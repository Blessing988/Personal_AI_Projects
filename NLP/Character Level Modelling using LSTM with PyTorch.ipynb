{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project involves training an `LSTM` on a document for Character level Modelling. \n",
    "\n",
    "The model is trained on the book by Jules Verne titled ` MYSTERIOUS ISLAND`, 1874"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset can be downloaded for the project [here](https://www.gutenberg.org/files/1268/1268-0.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing \n",
    "\n",
    "-  Remove irrelevant text such as Title\n",
    "- Copyright information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in the text: 1112300\n",
      "Number of unique characters in the text: 80\n"
     ]
    }
   ],
   "source": [
    "with open('data/1268-0.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "start_idx = text.find('THE MYSTERIOUS ISLAND')\n",
    "end_idx = text.find('END OF THE PROJECT GUTENBERG EBOOK THE MYSTERIOUS ISLAND') \n",
    "text = text[start_idx:end_idx]\n",
    "\n",
    "char_text = set(text)  # set of all unique characters in the text\n",
    "\n",
    "print('Number of characters in the text: {}'.format(len(text)))\n",
    "print('Number of unique characters in the text: {}'.format(len(char_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Mapping that converts between Characters and Integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text length: 1112300\n",
      "THE MYSTERIOUS  Encoded_text ---> [44 32 29  1 37 48 43 44 29 42 33 39 45 43  1]\n",
      "[33 43 36 25 38 28  1  6  6  6  0  0  0  0  0] Reversed Text ---> ISLAND ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorted_chars = sorted(char_text)  # sorting the characters\n",
    "char_array = np.array(sorted_chars)  \n",
    "char2int = {ch:i for i,ch in enumerate(sorted_chars)}  # dictionary that maps each character to a unique integer\n",
    "\n",
    "encoded_text = np.array([char2int[ch] for ch in text], dtype=np.int32)  # encoding the text\n",
    "\n",
    "print('Encoded text length:', len(encoded_text))\n",
    "\n",
    "print(text[:15], 'Encoded_text --->', encoded_text[:15])\n",
    "print(encoded_text[15:30], 'Reversed Text --->', ''.join(char_array[encoded_text[15:30]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T ---> 44\n",
      "H ---> 32\n",
      "E ---> 29\n",
      "  ---> 1\n",
      "M ---> 37\n",
      "Y ---> 48\n",
      "S ---> 43\n",
      "T ---> 44\n",
      "E ---> 29\n",
      "R ---> 42\n",
      "I ---> 33\n",
      "O ---> 39\n",
      "U ---> 45\n",
      "S ---> 43\n",
      "  ---> 1\n"
     ]
    }
   ],
   "source": [
    "for ch in text[:15]:\n",
    "    print(ch, '--->', char2int[ch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Dataset\n",
    "\n",
    "- Use a specified sequence length of say n\n",
    "- Divide the dataset into chunks of `n+1`\n",
    "- `some_encoded_text[:n]` will represent the input sequence\n",
    "- `some_encoded_text[1:n+1]` will represent the target sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CharDataset at 0x292c4198c10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = 40\n",
    "chunk_size = seq_len + 1\n",
    "chunks = len(encoded_text) // chunk_size\n",
    "\n",
    "text_chunks = [encoded_text[i:i+chunk_size] for i in range(len(encoded_text)-chunk_size)]\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, text_chunks):\n",
    "        self.text_chunks = text_chunks\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text_chunks)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text_chunk = self.text_chunks[idx]\n",
    "        return text_chunk[:-1].long(), text_chunk[1:].long()    \n",
    "\n",
    "char_dataset  = CharDataset(torch.from_numpy(np.array(text_chunks)))\n",
    "char_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Input:\n",
      " tensor([44, 32, 29,  1, 37, 48, 43, 44, 29, 42, 33, 39, 45, 43,  1, 33, 43, 36,\n",
      "        25, 38, 28,  1,  6,  6,  6,  0,  0,  0,  0,  0, 44, 32, 29,  1, 37, 48,\n",
      "        43, 44, 29, 42]) \n",
      "Encoded Target:\n",
      " tensor([32, 29,  1, 37, 48, 43, 44, 29, 42, 33, 39, 45, 43,  1, 33, 43, 36, 25,\n",
      "        38, 28,  1,  6,  6,  6,  0,  0,  0,  0,  0, 44, 32, 29,  1, 37, 48, 43,\n",
      "        44, 29, 42, 33])\n",
      "\n",
      "Input Token:  'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTER'\n",
      "Target Token:  'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTERI'\n"
     ]
    }
   ],
   "source": [
    "for i, (inp, target)  in enumerate(char_dataset):\n",
    "    print('Encoded Input:\\n', inp, '\\nEncoded Target:\\n', target)\n",
    "    print('\\nInput Token: ', repr(''.join(char_array[inp])))\n",
    "    print('Target Token: ', repr(''.join(char_array[target])))\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare our Dataset into  DataLoader and convert to batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "torch.manual_seed(42)\n",
    "\n",
    "char_dl = DataLoader(char_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.rnn  = nn.LSTM(embedding_dim, hidden_dim, \n",
    "                            batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden, cell):\n",
    "        ''' Forward pass through the network.'''\n",
    "        out = self.embedding(x).unsqueeze(1) # out.shape = (batch_size, seq_len, embedding_dim)\n",
    "        out, (hidden, cell) = self.rnn(out, (hidden, cell)) # out.shape = (batch_size, seq_len, hidden_dim)\n",
    "        out  = self.fc(out).reshape(out.size(0), -1) # out.shape = (batch_size, vocab_size)\n",
    "        return out, hidden, cell\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden and cell state '''\n",
    "        hidden = torch.zeros(1, batch_size, self.hidden_dim)\n",
    "        cell = torch.zeros(1, batch_size, self.hidden_dim)\n",
    "        return hidden, cell\n",
    "    \n",
    "\n",
    "# # example\n",
    "# # Parameters\n",
    "# vocab_size = 100\n",
    "# embedding_dim = 20\n",
    "# hidden_dim = 50\n",
    "\n",
    "# # Instantiate the model\n",
    "# rnn = RNN(vocab_size, embedding_dim, hidden_dim)\n",
    "\n",
    "# # Generate input data\n",
    "# batch_size = 64\n",
    "# seq_len = 40\n",
    "# x = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "# hidden = torch.zeros(1, batch_size, hidden_dim)\n",
    "# cell = torch.zeros(1, batch_size, hidden_dim)\n",
    "\n",
    "# # Pass data through the model\n",
    "# output = rnn.forward(x, hidden, cell)\n",
    "# output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(80, 256)\n",
       "  (rnn): LSTM(256, 512, batch_first=True)\n",
       "  (fc): Linear(in_features=512, out_features=80, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(char_array)\n",
    "embedding_dim = 256\n",
    "rnn_hidden_dim = 512\n",
    "model = RNN(vocab_size, embedding_dim, rnn_hidden_dim)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Modle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 4.3964\n",
      "Epoch 500 loss: 1.4802\n",
      "Epoch 1000 loss: 1.3792\n",
      "Epoch 1500 loss: 1.2563\n",
      "Epoch 2000 loss: 1.2549\n",
      "Epoch 2500 loss: 1.1840\n",
      "Epoch 3000 loss: 1.1640\n",
      "Epoch 3500 loss: 1.1858\n",
      "Epoch 4000 loss: 1.1623\n",
      "Epoch 4500 loss: 1.1688\n",
      "Epoch 5000 loss: 1.1145\n",
      "Epoch 5500 loss: 1.0829\n",
      "Epoch 6000 loss: 1.0705\n",
      "Epoch 6500 loss: 1.0246\n",
      "Epoch 7000 loss: 1.0460\n",
      "Epoch 7500 loss: 1.0393\n",
      "Epoch 8000 loss: 1.0241\n",
      "Epoch 8500 loss: 1.0220\n",
      "Epoch 9000 loss: 1.0274\n",
      "Epoch 9500 loss: 1.0265\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10000\n",
    "torch.manual_seed(42)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    hidden, cell = model.init_hidden(batch_size)\n",
    "    inp, target = next(iter(char_dl))\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0 \n",
    "\n",
    "    for ch in range(seq_len):\n",
    "        pred, hidden, cell  = model(inp[:,ch], hidden, cell)  # Use the teacher-forcing technique\n",
    "        loss += loss_fn(pred, target[:,ch])\n",
    "        # print(pred.shape)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss = loss.item() / seq_len\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f'Epoch {epoch} loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_state_dict.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate our Model\n",
    "\n",
    "- `Create a function that generates some output based on a starting sting`\n",
    "\n",
    "1. Initialization:\n",
    "\n",
    "- Start with an initial string `starting_str`.\n",
    "- Set generated_str to starting_str.\n",
    "\n",
    "2. Iteration:\n",
    "\n",
    "- Convert the recent part of generated_str to its encoded integer form.\n",
    "- Pass the encoded sequence into the RNN model to update its hidden state.\n",
    "- Let the model predict logits for the next possible character.\n",
    "- Use the `Categorical` class to sample a character based on these logits.\n",
    "- Append the sampled character to generated_str.\n",
    "\n",
    "3. Completion:\n",
    "\n",
    "- Continue iterating until `generated_str` reaches the desired length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, starting_str, len_generated_text=500,  scale_factor=1.00):\n",
    "    \"\"\"Generate the next sequence of texts using the LSTM model and a starting string.\n",
    "\n",
    "    Args:\n",
    "        model : LSTM model\n",
    "        starting_str (str): reference string to predict the next sequence of characters\n",
    "        len_generated_text (int, optional): The length of tokens to generate.\n",
    "        scale_factor (float, optional): The temperature to use.This controls the randomness of the predictions.\n",
    "\n",
    "    Returns:\n",
    "        str: Generates the completed sequences of characters\n",
    "    \"\"\"\n",
    "\n",
    "    # Encode the starting string into integers\n",
    "    encoded_input = torch.tensor([char2int[char] for \n",
    "                                char in starting_str])\n",
    "    encoded_input = encoded_input.reshape(1, -1) # or encoded_input.unsqueeze(0)\n",
    "\n",
    "    generated_str = starting_str\n",
    "\n",
    "    # Update the model's hidden state based on the starting string\n",
    "    model.eval() \n",
    "    hidden, cell = model.init_hidden(encoded_input.size(0))\n",
    "    for ch in range(len(starting_str) - 1):\n",
    "        _, hidden, cell = model(encoded_input[:,ch].view(1), hidden, cell)\n",
    "    \n",
    "    # Predict the next character based on the previous ones\n",
    "    last_char = encoded_input[:,-1]\n",
    "    for ch in range(len_generated_text):\n",
    "        logits , hidden, cell = model(last_char.view(1), hidden, cell)\n",
    "        \n",
    "        \n",
    "        logits = logits.squeeze(0) \n",
    "        scaled_logits = logits * scale_factor\n",
    "        dist = Categorical(logits=scaled_logits) # Construct a multinomial distribution over the classes\n",
    "        last_char = dist.sample() # Sample a character from the distribution\n",
    "        generated_str += str(char_array[last_char.item()]) # Append the sampled character to the generated text\n",
    "\n",
    "    return generated_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the colonists which water but the reporter\n",
      "found it not to keep him that if they could not go at once this time the settlers had about to goats, and he wished to goic with intentions of the liquid vast and twenty miles. Did not pass.\n",
      "\n",
      "Immeried himself used as the reporter told us very low tide. Their lips of circular distances they would restrain his reserve. “They then,” replied the reporter.\n",
      "\n",
      "“But reason there!” cried Pencroft. “But any traced a thing for anything anything besides, and they kanded an islen\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "gen_seq = sample(model, 'the colonists')\n",
    "print(gen_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the colonists were still flat with thick above the shore was the other substance from the balloon, and the two large end of the submarine forest, and he stretched out the cavern which had taken him as if it was impossible to his cheeks, looking at the end of the sea.\n",
      "\n",
      "The seaman should remain increased in the sea at the moment when the sea before his reserved for the castaways had been completely gallously from the mouth of the Mercy was going to his companions.\n",
      "\n",
      "“This is almost probably they may have taken \n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "gen_seq = sample(model, 'the colonists', scale_factor=2.0)\n",
    "print(gen_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Though increasing the temperature makes the generated text more diverse and random, <br>\n",
    "the text becomes less coherent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decreasing the temperature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the colonists ran two;\n",
      "neverth, for it had\n",
      "fought on, to leave lu. Forsts were micest plan.\n",
      "\n",
      "The hopefouvogigatings were yets uppabbated, grass,” cloved imporharezeic warm” stophes.\n",
      "\n",
      "On not like thissana curs trust?--asked, “neve he.\n",
      "\n",
      "Emvizualeh!! S5 heres a voice,lump, withlish thone--Willwn me,\n",
      "hill\n",
      "lem yourseaplored?\n",
      "Coffss Cyrus?”\n",
      " “True Sestoke irea Leckhan libeling.” shall therrire Hutable, judged that they samoqsionaryze, which\n",
      "forgottenda, they metreat?\n",
      "Yrus, or!”\n",
      "\n",
      "milest\n",
      "nothation; butatedia. FRvas \n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "gen_seq = sample(model, 'the colonists', scale_factor=0.5)\n",
    "print(gen_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decreasing the temperature makes the output more coherent but less predictable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do \n",
    "\n",
    "- Increase the `sequence length` to see its effects on the model\n",
    "- Train for more epochs\n",
    "- Might use a Transformer-based architecture for this same problem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyml-book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
